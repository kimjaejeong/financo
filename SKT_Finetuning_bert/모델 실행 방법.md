# SKT brain BERT 실행하기

<https://github.com/team-financo/phishing_prevention/blob/master/docs/skt_brain_bert_binary_classification.md>



- SKT_bert 압축 상태와 train.csv, public_test.csv 파일을 클라우드로 옮기기
  - nlp_implementation-master.zip 을옮기면 됨
- 클라우드에서 압축을 해제한다.
  - (sudo apt install unzip)  // 설치 했으면 건너뛰기.
  - unzip 파일명 ex) unzip nlp_implementation-master.zip

## 수정한 파일들

- data/my_train.txt (추가)
- data/my_test.txt (추가)
- build_dataset.py (수정)
- evaluate2.py (추가)
- model/metric2.py (추가)

### build_dataset.py

```
import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split
from utils import Config

# loading dataset
data_dir = Path("data")
filepath = data_dir / "my_train.txt" # 이름을 단순히 my_train.txt로 바꿨음
dataset = pd.read_csv(filepath, sep="\t").loc[:, ["document", "label"]]
dataset = dataset.loc[dataset["document"].isna().apply(lambda elm: not elm), :]
tr, val = train_test_split(dataset, test_size=0.2, random_state=777)

tr.to_csv(data_dir / "train.txt", sep="\t", index=False)
val.to_csv(data_dir / "validation.txt", sep="\t", index=False)

tst_filepath = data_dir / "my_test.txt" # 이름을 단순히 my_test.txt로 바꿨음
tst = pd.read_csv(tst_filepath, sep="\t").loc[:, ["document"]] # 우리 데이터에는 label이 없어서 레이블 컬럼을 지웠음
tst = tst.loc[tst["document"].isna().apply(lambda elm: not elm), :]
tst.to_csv(data_dir / "test.txt", sep="\t", index=False)

config = Config(
    {
        "train": str(data_dir / "train.txt"),
        "validation": str(data_dir / "validation.txt"),
        "test": str(data_dir / "test.txt"),
    }
)
config.save(data_dir / "config.json")
```

### evaluate2.py

같은 폴더 내에 있는 evaluate.py 파일을 cp 명령어를 이용해서 `cp evaluate.py evaluate2.py`로 복사를 한 다음에 수정했습니다.

```
import argparse
import pickle
import torch
import torch.nn as nn
from pathlib import Path
from torch.utils.data import DataLoader
from transformers.modeling_bert import BertConfig
from pretrained.tokenization import BertTokenizer as ETRITokenizer
from gluonnlp.data import SentencepieceTokenizer
from model.net import SentenceClassifier
from model.data import Corpus
from model.utils import PreProcessor, PadSequence
from model.metric2 import evaluate, acc # metric2 파일에서 evaluate 불러오기
from utils import Config, CheckpointManager, SummaryManager

parser = argparse.ArgumentParser()
parser.add_argument('--data_dir', default='data', help="Directory containing config.json of data")
parser.add_argument('--model_dir', default='experiments/base_model', help="Directory containing config.json of model")
parser.add_argument('--dataset', default='test', help="name of the data in --data_dir to be evaluate")
parser.add_argument('--type', default='skt', choices=['skt', 'etri'], required=True,  type=str)


if __name__ == '__main__':
    args = parser.parse_args()
    ptr_dir = Path('pretrained')
    data_dir = Path(args.data_dir)
    model_dir = Path(args.model_dir)

    ptr_config = Config(ptr_dir / 'config_{}.json'.format(args.type))
    data_config = Config(data_dir / 'config.json')
    model_config = Config(model_dir / 'config.json')

    # vocab
    with open(ptr_config.vocab, mode='rb') as io:
        vocab = pickle.load(io)

    # tokenizer
    if args.type == 'etri':
        ptr_tokenizer = ETRITokenizer.from_pretrained(ptr_config.tokenizer, do_lower_case=False)
        pad_sequence = PadSequence(length=model_config.length, pad_val=vocab.to_indices(vocab.padding_token))
        preprocessor = PreProcessor(vocab=vocab, split_fn=ptr_tokenizer.tokenize, pad_fn=pad_sequence)
    elif args.type == 'skt':
        ptr_tokenizer = SentencepieceTokenizer(ptr_config.tokenizer)
        pad_sequence = PadSequence(length=model_config.length, pad_val=vocab.to_indices(vocab.padding_token))
        preprocessor = PreProcessor(vocab=vocab, split_fn=ptr_tokenizer, pad_fn=pad_sequence)

    # model (restore)
    checkpoint_manager = CheckpointManager(model_dir)
    checkpoint = checkpoint_manager.load_checkpoint('best_{}.tar'.format(args.type))
    config = BertConfig(ptr_config.config)
    model = SentenceClassifier(config, num_classes=model_config.num_classes, vocab=preprocessor.vocab)
    model.load_state_dict(checkpoint['model_state_dict'])

    # evaluation
    filepath = getattr(data_config, args.dataset)
    ds = Corpus(filepath, preprocessor.preprocess)
    dl = DataLoader(ds, batch_size=model_config.batch_size, num_workers=4)

    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
    model.to(device)

    res = evaluate(model, dl, {'loss': nn.CrossEntropyLoss(), 'acc': acc}, device)
    
    # 평가하고 summary를 작성하는 부분 대신에 res.txt 파일로 저장함
    with open('res.txt', 'wt') as f:
        for i in range(len(res)):
            f.write(str(res[i]) + '\r\n')
```

### model/metric2.py

```
import torch
import numpy as np
from IPython import embed
from tqdm import tqdm


def evaluate(model, data_loader, metrics, device):
    if model.training:
        model.eval()

    res = [] # 결과를 반환할 리스트를 생성
    for step, mb in tqdm(enumerate(data_loader), desc='steps', total=len(data_loader)):
        x_mb, y_mb = map(lambda elm: elm.to(device), mb)

        with torch.no_grad():
            y_hat_mb = model(x_mb)
            # 모델에서 나온 값을 softmax해줘서 확률로 변환한 다음 결과 리스트에 저장
            res += y_hat_mb.softmax(dim=1)[:,1].tolist()
    # 결과 값을 반환
    return res


def acc(yhat, y):
    with torch.no_grad():
        yhat = yhat.max(dim=1)[1]
        acc = (yhat == y).float().mean()
    return acc
```

## 실행 순서

1. `(my_train.txt`와 `my_test.txt`파일을 생성해서 data/ 폴더에 집어넣습니다.

   - 폴더에 다음 코드를 작성하여 넣어줍니다.)

     ```python
     import pandas as pd
     df = pd.read_csv('train.csv')
     df2 = pd.read_csv("public_test.csv")
     df_new = pd.DataFrame()
     df2_new = pd.DataFrame()
     df_new['document'] = df.text
     df2_new['document'] = df2.text
     df_new['label'] = df.smishing
     df_new.to_csv('my_train.txt', index=False, sep='\t')
     df2_new.to_csv("my_test.txt", index=False, sep='\t')
     ```

     - df2 즉, test의 경우 label이 없으므로 df2_new['label'] 과정을 진행하지 않습니다.
     
   - 만약 처음에 진행했다면 2번부터 진행하면 됩니다.

2. 파일들을 위와 같이 수정한 다음, `python3 build_dataset.py` 파일을 실행해서 train, test, valid set을 분리합니다.

3. python3 prepare_vocab_and_weights.py --type=skt 을 실행해서 준비합니다.

   - vocab 과정과 초기 weights를 준비하게금 해줍니다.

   - 만약,  transformers가 설치 되어있지 않다고 나오면 다음과 같이 설치합니다.

     ```python
     pip3 install transformers==2.2.2 --user
     ## pip transformers 2.3.0으로 설치 하기 때문에 버전을 바꾸자!! downgrade를 이루어야 함.
     ```

   - 설치는 총 3~4개가 나올 것입니다.

     - pip3 install gluonnlp --user
     - pip3 install mxnet --user

4. `python3 train.py --type=skt` 명령으로 학습을 진행합니다.

5. `python3 evaluate2.py --type=skt` 명령을 실행한 후, `res.txt`를 로컬로 다운받아서 결과를 봅니다.

6. 이후 txt 파일이 나오면 원하는 형태로 csv로 만든 뒤 제출을 합니다.

(TODO: 조금 더 자동화 해서 만들어 놓기)

