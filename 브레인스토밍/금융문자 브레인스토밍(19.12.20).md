## 용어정리

- 임베딩
  - 정의
    - 단어, 문장 -> 숫자
  - 예시
    - word2vec /glove / fast text / CBOW / Skip gram
      - 단어 단위 임베딩
    - elmo / bert / GPT
      - 문장 단위 임베딩

- 토크나이징
  - 정의
    - 어떤 토큰을 취하느냐에 따라 입장이 다름.
  - 예시
    - 단어 단위 토크나이징 
    - 문자 단위 토크나이징
- vocab

## 그외

- bert 
  - wordpiece / BPE
    - wordpiece를 만들 때 BPE로 만든 것인데, wordpiece는 bert의 전처리이다.
  - bert는 UNK라는 토큰으로. UNK를 줄일 수 있는 방법을 찾아야 함.
    - 줄이기 위한 방법은 형태소 분석을 잘해야 함.
  - ##을 넣은 이유는 첫 번째 문자랑 구분하기 위해.
  - 첫 번째 문자는 중요하게 생각하는 듯.
- ETRI KorBERT를 위한 Open 형분석기
  - mecab
    - 일본에서 만듬.
  - Khaii
    - 카카오에서 제작.
    - CNN 기반으로 형태소로 나눈 것.
  - Soy NLP
  - google sentence piece

- 우리 bert
  - Etri bert
  
    - 다른 bert 모델처럼 ##을 붙이는 것이 아니라 끝에 _를 붙임.
  
      (https://youtu.be/PzvKDpQgNzc - 3분 28초 참고)
  
    - == KorBERT
  
  - SKT brain => SKT bert
  
    - == KoBERT
  
  - 한국어 임베딩 => 이기창 bert
  
  - 동영상 강의 => 솔트룩스 bert
  
  - 개인 bert

## 프로젝트 예상 과정

### BaseLine

형태소(open형) 분석기 진행 -> KoBert 학습을 하여 Fine tuning 진행

- Khaii를 활용하여 가지고 있는 데이터를 통해 vocab을 만들고
- etri cobert / SKT brain bert / 구글 bert multi language / 개인이 만든 Kobert  중 하나를 선택하여 Finetuning 진행.
  - etri cobert와 구글 bert multi language가 가장 괜찮다고 함.

### 이후 

