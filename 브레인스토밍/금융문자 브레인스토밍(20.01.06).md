## Basaeline 이후 왜 일정 수치에서 성능이 오르지 않을까?

- 공개된 SKT_BERT에서 token length가 제한이라는 것을 발견함.
  - 발견 계기
    - XXX고XXX객 에서 XXX를 제거했을 때, 0.1 이상 떨어지는 모습을 확인했다. 이것이 착안이 되어, token_length를 확인 하였더니, 들어가는 것이 갯수가 확연히 달라지는 모습을 확인했다.
    - 하나의 문장만 넣었는데 token이 중간에 끊기는 현상을 발견.
    - 기존 token_length가 64밖에 없다는 것을 알게 되었고, 그 이후 문장들은 무시하게 됨..
    - 사람들이 V100을 32개로 하는 이유가 학습 시간을 줄이는 것 뿐만 아니라, token size의 값을 늘리기 위해서이기도 함.
  - 적용
    - 우리는 token_length를 늘리기 위해 GPU 개수를 추가함.
      - 병렬로 로딩하는 방법 활용
- 학습 방법 노하우 적용
  - 우리는 돈이 없기 때문에 512 성능을 내야 한다.
  - 솔트룩스 버트 강의에서 128 -> 512로 올리라고 이야기 했음.
    - 처음엔 128로 학습을 진행 90퍼센트 정도 학습을 했다고 했을 때, 마지막 10퍼센트는 512로 학습
      그러면 버트 성능이 512로 학습이 되면서, 학습 속도가 훨씬 줄게 됨.
      max_seq_length는 preprocessed data를 만들 때와 실제 학습할 때와는 일치  해야 함. 만약 128을 90%정도 진행했을 때, max_seq_length=128로 한 preprocessed data를
      만들어 놓고, 512로 한 data를 만들어야 함. 그다음 버트 학습을 진행할 때, 128짜리로 먼저 학습을 진행하고, 그다음에 512짜리로 연장학습을 진행해야 함.
    - 솔트룩스는 pretraining 하는 것이고, 우리는 finetuning 하는 것임.
- 독립된 vocab_size를 만들면, 독립된 갯수를 config.json에서 수정해줘야 함. 나중에 word embedding layer를 생성하게 됨. vocab_size와 word_embedding layer가 맞지 않으면
  에러가 됨.

## 역할

재정

- 동욱이형 맞춤법 돕기

동욱

- SKT_BERT에서 token_length가 제한된다는 것을 발견함.

지운

- okt / khaii로 형태소 나눈 후, 블랙리스트 추가 
- 블랙리스트 확인 방법
  - OKT 단어 종류가 총 2000개 정도 나왔고, 그 단어들 중 1,000개 이상인 것만 블랙리스트로 지정.
  - 블랙리스트 단어는 총 20개 정도이고 이 단어가 포함된 문장들은 모두 1로 설정.
    - 예를들어, '안녕하세요 감사하무니다' 에서 블랙리스트가 '감사하무니다' 일 경우에는 문장이 1로 되는 것이고, '하무니다'가 블랙리스트면 1이 되지 않는 것이다.

경희

- token을 합치는 방법
  - 안녕하세요를 하나로 치환해야 하는지.



## 앞으로 해볼 성능 높이는 방법

- stopwords를 없애는 것이 중요할듯.
- 조사 없애고 확인

## 최적 성능 높이는 방법

- BlackList + 끝맺음 처리(이상한문자, 안녕하세요) +  token_length 늘리기



## 앞으로 할 것

- 토큰 공부
- 당근마켓 코드 활용
- 병렬컴퓨팅
  - 병렬일 때는 불러오는 방식이 달라짐.
- distillation Bert  알아보기
- t5 논문 알아보기



## 내가 할 것

- 동욱이형 코드 전처리 코드 만들기





## 추후에 할 일

- 스미싱 앱 만들기
  - .pv
  - pytorch -> tensorflow로 바꿔야 함.
  - JAVA or 코틀린