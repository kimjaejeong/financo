## 언어 모델

- ML기반
  - 확률기반
  - 예시
    - 베이지안
      - 조건부 확률로 다음 것을 예측
      - 단점
        - 전에 것을 알아야 함.
    - SVM
      - 
      - 단점
        - 속도가 느림

- DL기반

  - 신경망 기반

  - 순서

    : DNN -> RNN -> LSTM, GRU -> seq2seq -> attention -> Elmo, GPT -> Bert -> XLNET

    - seq2seq

      : input -> context vector -> output

      - input -> context vector는 encoder
      - context vector -> output은 decoder

      - input -> context vector에서는 맨 마지막만 학습

    - attention

      - seq2seq에 불만. 왜냐하면 맨 마지막만 학습하기 때문에.
      - input -> context vector에서 맨 마지막이 아닌 하나씩 학습함

    - (self attention)

      - 이때 부터 transfer라고 부르고 Elmo, GPT부터 이것을 이용함.

    - Elmo, GPT

      - AutoRegression
        - 순서대로 학습하여 예측
      - self attention 기반

    - Bert(Bidirectional Encoder Representations from Transformers)

      - Auto Encoding
        - 한 번에 읽어서 예측함.
      - mutl head attention 기반
      - 현재 Roberta가 auto encoding의 최신이라고 함.

    - XLNET

      - AutoRegression
        - autoregression인 이유는 Bert에서는 한 번에 읽어서 빈 칸을 예측 해야 하는데, 빈 칸의 관계는 알지 못한다.
        - 빈 칸 간의 관계까지 파악하기 위해서는 autoRegression이 사용됨.



## 도전해야할 것

- Bert 한글은 있지만, 전처리를 어떻게 잘해야 할까? / fine tuning
- XLNET pretraining R&D 영역.