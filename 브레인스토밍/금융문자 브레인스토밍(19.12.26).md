- 그동안 결과가 제대로 나오지 않았던 이유를 발견함
  
  - 우리는 단순히 0과 1로만 제출하는 줄 알았는데, **smishing 변수의 각 예측값 확률**을 만들어 계산해야 했다.
  
  - ![1577328468466](C:\Users\u37c\AppData\Roaming\Typora\typora-user-images\1577328468466.png)
  
    - <https://dacon.io/index.php?mid=cpt14&document_srl=226611&cpage=1#comment>
  
      (알렉스 댓글 참고)
  
  - 참고예시
  
    <https://cyc1am3n.github.io/2018/11/10/classifying_korean_movie_review.html>
  
    ([Keras] KoNLPy를 이용한 한국어 영화 리뷰 감정 분석)
  
    ```python
    model.compile(optimizer=optimizers.RMSprop(lr=0.001),
                 loss=losses.binary_crossentropy,
                 metrics=[metrics.binary_accuracy])
                 
    # 출현 빈도가 높은 상위 토큰 10개
    print(text.vocab().most_common(10))
    ```

- 동욱이형 왈) data 보면서
  - training data에서 smishing인지 아닌지를 잘 봐야할듯.
  - AUC 0.8~ 0.94!!!!
  - 전처리를 어떻게 할지??
  - 특수문자, 숫자, 
  - stopwords, stemming
  - mecab으로 형태소 분석을 하면 
    - pos tagging을 하면서 빈도수 높은거, 적은거
      - 빈도수 낮은거는 날려버려야 함.
    - 단순히 형태소 단위로 분리
    - 어떤 특성을 가져야 할지.?
    - 형태소 파악 후, 빈도수 조사. 빈도수 낮은거는 날려버리기.
    - 비슷한 단어끼리 묶어야 할 것.
    - 특수문자...
- text 특징 분석
  - 띄어쓰기 + 글자 공간 확인
    - XXX 처리 확인
  - 은어, 비속어 등이 없어서 Bert가 이해하기 쉬울 것 같음.
  - 날짜 통합
    - 107행에서 2월 1일 9시 20분 / 112행에서 2017-01-16과  
      - 날짜를 'date' 형태로 통합.
  - 어둔 -> 어두운?
    - 152행
  - 203행 !!! 갯수 통합
  - XXX는 삭제
  - (ㅡ)는 띄어쓰기로 바꿔줘야 할듯.
  - Mecab, Kahii 설치해서 training data를 통해 

- 전처리 하는 이유
  - EMBEDDING이 제한
  - 제한된 길이 안에 정규화를 하는 느낌으로.

- 자료공유
  - 동욱이형
    - mecab 전처리 + Dacon 전처리 + bert 모델
      - Dacon 전처리
        - tokenizer.pos() 



- smishing 특징 분석
  - CSS - 개인신용평가
  - url



- 오늘 할 것 
  - mecab 설치
  - 콜랩을 통해, 빈도수 조사
  - 