모델 후 -> 전처리

- base라인 모델
  - tree
- 병목
  - 전처리 - 모델
  - 학습 시간
    - 실패해도 빠르게 학습...

할 일

- SK C&C / ETRI 조사

  - ##### 한국어 bert

- NLP Classfication SOTA 확인
  
- 한국어 pre-trained... 
  
- 클라우드 안쓴 사람 모으기

  - 정우, 경희누나??
  - TPU

- seq2seq

- XLNET 알아보기

  - Bert 다음에 나온 것.

- BCS / TPU 돌아갈지?

- 전처리는 괜찮은데, 모델을 TPU / GPU가 support가 되는지?

  - 왜냐하면 케라스, 텐서플로우, pytorch 호환성을 확인.

- ##### 한국어 bert

- ##### 모델 TPU

- ##### 클라우드 TPU

- SOTA?

- seq2seq
  - 중심 -> 주변
  - 주변 -> 중심

- ulmfit
- Bert Large
- auto ML!!!
  - 파라미터 자동으로 지정 가능
  - 구글이나 아마존에 다 있음.

- Grid Search
  
  - 모델만 튜닝할 때
- 박길식강사님 자연어처리 방법 다시 보기
- 한국어 embedding
  - embedding은 집약되어서 각각 값들을 조합해서 의미를 만들어낸다.
  - sparse matrix
  - dense matrix
  - context vector가 embedding이 된 것인데, embedding을 만드는 과정.

- GPU / TPU 차이





성능이 잘 나오는 방법

- 한국어 embedding이 잘 되어있어야 함.
  - 분류가 잘 될 것임.

- Bert vs XLNET vs umlfit 선택
  - ELMO / google GPT
  - Bert
    - Bert부터 제대로 양방향
    - Transfer Learning mode



내가 할 일

- A반 인원 돌리기
- 클라우드 안쓴 사람 모으기.
- 일정 확인
- github 관리
  - master branch
  - role branch
  - feature branch
- 팀 제출 언제까지인지 알아보기



진행방법

- 한국어 임베딩 체크 확인

- 타당성 검증
  - 클라우드 구축 -> 전처리 대충 -> Bert or XLNET -> 평가
    - Embedding
    - 클라우드 구축
      - TPU GPU 결정
    - 전처리
      - 띄어쓰기
      - 조사
      - 어근추출
    - 임베딩
    - 모델 학습
      - 학습
        - bert 기본 튜닝
      - 평가

- 30만개 전부 돌리기

- 이후, 클라우드, 전처리, 모델 조합 선택 
  - 클라우드 구축 -> 전처리 -> 모델링 학습 -> 평가
    - 클라우드
      - TPU 찾기 위함.
      - 종류
        - GCP
        - AWS
        - Azure
    - 전처리
      - 어근
      - 조사
      - 띄어쓰기
      - 특수문자
      - 명사처리
    - (임베딩)
      - dynamic일 경우에는 할 필요 없음.
      - glove
      - fast text
    - 모델학습
      - 

질문거리

- 모델 테스트를 어떤식으로 하나?
  - TPU or GPU??

업무환경

- trello - 권한은 전부
  - todo
    - 중요업무
      - baseline 구축 안에 들어가야 할 것.
    - 일반업무
      - 어느정도 baseline 완성 후 umlfit 더 알아보기
  - done
  - 공지
    - 사람들이 이건 알았으면 좋겠다.
  - 공유드라이브 / git url
- Slack
  - auto deploy



공부

- seq2seq(RNN -> LSTM -> GRU) -> attention -> self attention -> multi head attention -> ulmfit, elmo -> bert -> XLNET 
  - multi head attention은 attention이 여러개.
  - self attention부터가 transfer model임.
  - bert는 multi head attention을 기반으로 사용.

