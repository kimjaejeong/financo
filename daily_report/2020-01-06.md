# 2020-01-06

### Done

- Basaeline 이후 왜 일정 수치에서 성능이 오르지 않을까?
  - 공개된 SKT_BERT에서 token length가 제한이라는 것을 발견
    - 발견 계기
      - XXX고XXX객 에서 XXX를 제거했을 때, 0.1 이상 떨어지는 모습을 확인. 이것을 착안하여 token_length를 확인 하였더니, token에 들어가는 갯수가 매우 적은 것을 확인.
    - 적용
      - 우리는 token_length를 늘리기 위해 GPU 개수를 추가
      - 솔트룩스 버트 강의에서 128 -> 512로 올리라고 이야기
        - 처음엔 128로 학습을 진행 90퍼센트 정도 학습을 했다고 했을 때, 마지막 10퍼센트는 512로 학습
          그러면 버트 성능이 512로 학습이 되면서, 학습 속도가 훨씬 줄게 됨.
          max_seq_length는 preprocessed data를 만들 때와 실제 학습할 때와는 일치  해야 함. 만약 128을 90%정도 진행했을 때, max_seq_length=128로 한 preprocessed data를
          만들어 놓고, 512로 한 data를 만들어야 함. 그다음 버트 학습을 진행할 때, 128짜리로 먼저 학습을 진행하고, 그다음에 512짜리로 연장학습을 진행
        - 솔트룩스는 pretraining 하는 것이고, 우리는 finetuning 하는 것
- Okt / Khaii로 형태소 나눈 후, 블랙리스트 추가
  - 블랙리스트 확인 방법
    - Okt 단어 종류가 총 2000개 정도 나왔고, 그 단어들 중 1,000개 이상인 것만 블랙리스트로 지정.
    - 블랙리스트 단어는 총 20개 정도이고 이 단어가 포함된 문장들은 모두 1로 설정.
      - 예를들어, '안녕하세요 감사하무니다' 에서 블랙리스트가 '감사하무니다' 일 경우에는 문장이 1로 되는 것이고, '하무니다'가 블랙리스트면 1이 되지 않음.



