# 2020-01-07

### Done

- pretraining vs 기존
  - pretraining
    - 도전적이고, 많이 배울 수 있음.
    - 그러나, 시간이 부족함.
  - 기존
    - 충분히 0.99 달성 가능성이 높아보였음.
    - 따라서 우리는 반드시 top20 안으로 들어가자!
    - 따라서 기존 방식을 선정.
- 0.981706 달성
  - token_length = 160, 데이터를 뒤집어서 넣었더니 버트가 학습을 잘했음.
  - +여기에 okt 형태소 분석을 활용하여 blacklist 전체를 접목하니 0.981706이 나왔음.
  - 동욱 생각은 뒤 쪽에 스미싱 피쳐가 많이 있어서 잘 잡히지 않았을까라는 생각.
    - 따라서 앞 쪽에는 많이 없기 때문에 뒤쪽 보다 baseline auc가 낮게 나온 것이라고 추측.

- okt, khaii 전체 blacklist가 성능이 좋게 나왔음.
  - okt > khaii

- 경희 왈, 이메일 피쳐는 잘 성립되지 못했음.
- 0.99 가즈아~